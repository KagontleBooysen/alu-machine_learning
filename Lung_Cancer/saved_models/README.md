Trained and tested a ML model for predicting lung cancer & analyzed various results in form of histogram, subplot, scatterplot, bar graph . Compared accuracy of model using algorithms such as Decision tree, Logistic Regression , KNN , Random Forest and SVM. Plotted training

In optimizing the neural network for the project, several key parameters were carefully considered and tuned to enhance model performance. Stochastic Gradient Descent (SGD) was employed with a learning rate of 0.01 to balance convergence speed and stability during training. A batch size of 32 was chosen for the Mini-Batch Gradient Descent to ensure computational efficiency and stable model updates. Learning rate scheduling was implemented with a time-based decay strategy, reducing the learning rate after 15 epochs to fine-tune convergence. Adaptive Learning Rate Methods, specifically Adam, featured a learning rate of 0.01, Beta1 of 0.9, Beta2 of 0.999, and an epsilon value of 1e-8, providing stability and effective adaptation during training. L2 regularization with a lambda value of 0.01 was utilized for regularization purposes, preventing overfitting by penalizing large weights. Dropout, with a rate of 0.5, was implemented to further prevent overfitting by randomly dropping neurons during training. Early stopping, set with a patience of 5 epochs, helped balance training time and prevent overfitting by halting when no improvement was observed. The parameter tuning process involved grid search, random search, and cross-validation, ensuring a thorough exploration of parameter spaces. Empirical evaluation and iterative experimentation played a pivotal role, with constant monitoring of metrics guiding the selection of optimal parameter values for a well-tailored and effective neural network model.
